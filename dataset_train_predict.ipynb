{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedba150-8499-452c-a0db-777f5600fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# DATA_DIR = './data'\n",
    "# if not os.path.exists(DATA_DIR):\n",
    "#     os.makedirs(DATA_DIR)\n",
    "\n",
    "# number_of_classes = 48\n",
    "# dataset_size = 100\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# for j in range(number_of_classes):\n",
    "#     if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
    "#         os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
    "\n",
    "#     print('Collecting data for class {}'.format(j))\n",
    "\n",
    "#     done = False\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,\n",
    "#                     cv2.LINE_AA)\n",
    "#         cv2.imshow('frame', frame)\n",
    "#         if cv2.waitKey(25) == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     counter = 0\n",
    "#     while counter < dataset_size:\n",
    "#         ret, frame = cap.read()\n",
    "#         cv2.imshow('frame', frame)\n",
    "#         cv2.waitKey(25)\n",
    "#         cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
    "\n",
    "#         counter += 1\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6214957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5549b1-4977-4ec7-afdc-822abc9304b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "# Path to the parent directory containing subdirectories '0', '1', '2', etc.\n",
    "DATA_DIR = 'H:/MotionSpeak/hindi/tata'\n",
    "data = []\n",
    "labels = []\n",
    "for subdir in os.listdir(DATA_DIR):\n",
    "    subdir_path = os.path.join(DATA_DIR, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for img_path in os.listdir(subdir_path):\n",
    "            data_aux = []\n",
    "            x_ = []\n",
    "            y_ = []\n",
    "            img_file_path = os.path.join(subdir_path, img_path)\n",
    "            \n",
    "            # Check if the file exists\n",
    "            if os.path.isfile(img_file_path):\n",
    "                img = cv2.imread(img_file_path)\n",
    "                # Check if the image is not empty\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    results = hands.process(img_rgb)\n",
    "                    if results.multi_hand_landmarks:\n",
    "                        for hand_landmarks in results.multi_hand_landmarks:\n",
    "                            for i in range(len(hand_landmarks.landmark)):\n",
    "                                x = hand_landmarks.landmark[i].x\n",
    "                                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                                x_.append(x)\n",
    "                                y_.append(y)\n",
    "\n",
    "                            for i in range(len(hand_landmarks.landmark)):\n",
    "                                x = hand_landmarks.landmark[i].x\n",
    "                                y = hand_landmarks.landmark[i].y\n",
    "                                data_aux.append(x - min(x_))\n",
    "                                data_aux.append(y - min(y_))\n",
    "\n",
    "                        data.append(data_aux)\n",
    "                        labels.append(subdir)  # Use the directory name as label\n",
    "                else:\n",
    "                    print(f\"Skipping empty image: {img_file_path}\")\n",
    "            else:\n",
    "                print(f\"Skipping non-existent file: {img_file_path}\")\n",
    "\n",
    "f = open('datapickle', 'wb')\n",
    "pickle.dump({'data': data, 'labels': labels}, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfc5b6c-eed1-425e-9aa0-6c19dcc90557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.98989898989899% of samples were classified correctly !\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_dict = pickle.load(open('./datapickle', 'rb'))\n",
    "filtered_data = []\n",
    "filtered_labels = []\n",
    "\n",
    "# Iterate through the data and labels\n",
    "for i in range(len(data_dict[\"data\"])):\n",
    "    if len(data_dict[\"data\"][i]) == 42:\n",
    "        # Append the data point and label if the size is 42\n",
    "        filtered_data.append(data_dict[\"data\"][i])\n",
    "        filtered_labels.append(data_dict[\"labels\"][i])\n",
    "\n",
    "# Update data_dict with the filtered data and labels\n",
    "data_dict[\"data\"] = filtered_data\n",
    "data_dict[\"labels\"] = filtered_labels\n",
    "data = np.asarray(data_dict['data'])\n",
    "labels = np.asarray(data_dict['labels'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, shuffle=True, stratify=labels)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "\n",
    "print('{}% of samples were classified correctly !'.format(score * 100))\n",
    "\n",
    "f = open('model_01.p', 'wb')\n",
    "pickle.dump({'model': model}, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995b4e6b-2fc3-4f90-8f6e-439b5c01a3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1483"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f585ee0e-8940-4491-aa14-e4253e49ee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1483"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a4a10-1fc9-43e1-822f-b0ccc1f07aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import cv2\n",
    "# import mediapipe as mp\n",
    "# import numpy as np\n",
    "\n",
    "# model_dict = pickle.load(open('./model_01.p', 'rb'))\n",
    "# model = model_dict['model']\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# mp_hands = mp.solutions.hands\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# labels_dict = labels ={0:'A',1:'AA',2:'ae',3:'aee',4:'ana',5:'Ba',6:'Bha',7:'Cha', 8:'Chha',9:'Da', 10:'Daa',11:'Dha',12:'Dhha', 13:'e', 14:'ee',15:'Fa',16:'Ga',17:'Gha',18:'Gya',19:'Ha',20:'Hello',21:'iloveyou',22:'Ja',23:'Jha',24:'Ka',25:'Kha',26:'La', 27:'Lae', 28:'Ma',29:'Na',30:'Namaste',31:'no',32:'oo',33:'Pa', 34:'Ra',35:'Sa',36:'Sh',37:'Shaa',38:'Ta',39:'Th',40:'Tha',41:'Thanks',42:'Tta',43:'uu',44:'Uuu',45:'Va',46:'Ya',47:'Yes'}\n",
    "# while True:\n",
    "\n",
    "#     data_aux = []\n",
    "#     x_ = []\n",
    "#     y_ = []\n",
    "\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     H, W, _ = frame.shape\n",
    "\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     results = hands.process(frame_rgb)\n",
    "#     if results.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 frame,  # image to draw\n",
    "#                 hand_landmarks,  # model output\n",
    "#                 mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "#                 mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "#                 mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             for i in range(len(hand_landmarks.landmark)):\n",
    "#                 x = hand_landmarks.landmark[i].x\n",
    "#                 y = hand_landmarks.landmark[i].y\n",
    "\n",
    "#                 x_.append(x)\n",
    "#                 y_.append(y)\n",
    "\n",
    "#             for i in range(len(hand_landmarks.landmark)):\n",
    "#                 x = hand_landmarks.landmark[i].x\n",
    "#                 y = hand_landmarks.landmark[i].y\n",
    "#                 data_aux.append(x - min(x_))\n",
    "#                 data_aux.append(y - min(y_))\n",
    "\n",
    "#         x1 = int(min(x_) * W) - 10\n",
    "#         y1 = int(min(y_) * H) - 10\n",
    "\n",
    "#         x2 = int(max(x_) * W) - 10\n",
    "#         y2 = int(max(y_) * H) - 10\n",
    "\n",
    "#         prediction = model.predict([np.asarray(data_aux[:42])])\n",
    "\n",
    "#         predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "#         cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "#                     cv2.LINE_AA)\n",
    "\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1098b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.98989898989899% of samples were classified correctly !\n"
     ]
    }
   ],
   "source": [
    "print('{}% of samples were classified correctly !'.format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab00220",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_character' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 85\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Write the output to a file\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 85\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mpredicted_character\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_character' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "model_dict = pickle.load(open('./model_01.p', 'rb'))\n",
    "model = model_dict['model']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "\n",
    "labels_dict = {\n",
    "    0: 'अ', 1: 'आ', 2: 'ए', 3: ' ऐ', 4: 'ण', 5: 'ब', 6: 'भ', 7: 'च', 8: 'छ', 9: 'ड',\n",
    "    10: 'द', 11: 'ढ', 12: 'ध', 13: 'इ', 14: 'ई', 15: 'फ', 16: 'ग', 17: 'घ', 18: 'ज्ञ',\n",
    "    19: 'ह', 20: 'हैलो', 21: 'मैं तुमसे प्यार करता हूँ', 22: 'ज', 23: 'झ', 24: 'क', 25: 'ख', 26: 'ल',\n",
    "    27: 'ळ', 28: 'म', 29: 'न', 30: 'नमस्ते', 31: 'नहीं', 32: 'औ', 33: 'प', 34: 'र',\n",
    "    35: 'स', 36: 'क्ष', 37: 'श', 38: 'त', 39: 'थ', 40: 'ठ', 41: 'धन्यवाद', 42: 'ट',\n",
    "    43: 'उ', 44: 'ऊ', 45: 'व', 46: 'य', 47: 'हाँ'\n",
    "}\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux[:42])])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the output to a file\n",
    "with open('output.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(predicted_character)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dae3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading frame from the camera.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predicted_character' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Write the output to a file\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 63\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mpredicted_character\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_character' is not defined"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if frame is None:  # Check if frame is None\n",
    "        print(\"Error reading frame from the camera.\")\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            for i in range(len(hand_landmarks.landmark)):\n",
    "                x = hand_landmarks.landmark[i].x\n",
    "                y = hand_landmarks.landmark[i].y\n",
    "                data_aux.append(x - min(x_))\n",
    "                data_aux.append(y - min(y_))\n",
    "\n",
    "        x1 = int(min(x_) * W) - 10\n",
    "        y1 = int(min(y_) * H) - 10\n",
    "\n",
    "        x2 = int(max(x_) * W) - 10\n",
    "        y2 = int(max(y_) * H) - 10\n",
    "\n",
    "        prediction = model.predict([np.asarray(data_aux[:42])])\n",
    "\n",
    "        predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "        cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Write the output to a file\n",
    "with open('output.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(predicted_character)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
